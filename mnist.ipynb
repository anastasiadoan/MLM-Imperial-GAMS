{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import gamspy as gp\n",
    "from gamspy.math.matrix import dim\n",
    "\n",
    "from torch.optim import SGD\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as ptl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.MNIST('mnist_data', train=True, download=True, transform= ToTensor() )\n",
    "test_data = torchvision.datasets.MNIST('mnist_data', train=False, download=True, transform= ToTensor() ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = {\n",
    "    'train' : DataLoader(train_data, batch_size=100, shuffle=True, num_workers=1),\n",
    "    'test' : DataLoader(test_data, batch_size=100, shuffle=True, num_workers=1)  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x165360e20>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, 3, padding=1) \n",
    "        self.fc1 = nn.Linear(10 * 28 * 28, 10)  \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  # Convolutional layer with ReLU activation\n",
    "        x = x.reshape(-1, 10 * 28 * 28)  # Flatten the tensor\n",
    "        x = self.fc1(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_index, (data,target) in enumerate(loaders['train']):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_index % 20 == 0:\n",
    "            print(f'Train Epoch: (epoch) [{batch_index * len(data)}/{len(loaders[\"train\"].dataset)} ({100. * batch_index / len(loaders[\"train\"]):.0f}%)]\\t{loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']: \n",
    "            data, target = data.to(device), target.to(device) \n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim = True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(loaders['test'].dataset)\n",
    "\n",
    "    print(f\"\\nTest set: Average loss: {test_loss : .4f}, Accuracy {correct}/ {len(loaders['test'].dataset)} ({100. * correct / len(loaders['test'].dataset):.0f}%\\)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: (epoch) [0/60000 (0%)]\t2.308247\n",
      "Train Epoch: (epoch) [2000/60000 (3%)]\t1.287032\n",
      "Train Epoch: (epoch) [4000/60000 (7%)]\t0.588230\n",
      "Train Epoch: (epoch) [6000/60000 (10%)]\t0.467632\n",
      "Train Epoch: (epoch) [8000/60000 (13%)]\t0.367330\n",
      "Train Epoch: (epoch) [10000/60000 (17%)]\t0.339775\n",
      "Train Epoch: (epoch) [12000/60000 (20%)]\t0.261881\n",
      "Train Epoch: (epoch) [14000/60000 (23%)]\t0.300744\n",
      "Train Epoch: (epoch) [16000/60000 (27%)]\t0.276331\n",
      "Train Epoch: (epoch) [18000/60000 (30%)]\t0.069315\n",
      "Train Epoch: (epoch) [20000/60000 (33%)]\t0.400274\n",
      "Train Epoch: (epoch) [22000/60000 (37%)]\t0.116459\n",
      "Train Epoch: (epoch) [24000/60000 (40%)]\t0.156799\n",
      "Train Epoch: (epoch) [26000/60000 (43%)]\t0.176273\n",
      "Train Epoch: (epoch) [28000/60000 (47%)]\t0.229542\n",
      "Train Epoch: (epoch) [30000/60000 (50%)]\t0.127745\n",
      "Train Epoch: (epoch) [32000/60000 (53%)]\t0.124477\n",
      "Train Epoch: (epoch) [34000/60000 (57%)]\t0.224569\n",
      "Train Epoch: (epoch) [36000/60000 (60%)]\t0.145431\n",
      "Train Epoch: (epoch) [38000/60000 (63%)]\t0.187607\n",
      "Train Epoch: (epoch) [40000/60000 (67%)]\t0.121047\n",
      "Train Epoch: (epoch) [42000/60000 (70%)]\t0.121701\n",
      "Train Epoch: (epoch) [44000/60000 (73%)]\t0.187408\n",
      "Train Epoch: (epoch) [46000/60000 (77%)]\t0.201807\n",
      "Train Epoch: (epoch) [48000/60000 (80%)]\t0.089423\n",
      "Train Epoch: (epoch) [50000/60000 (83%)]\t0.181101\n",
      "Train Epoch: (epoch) [52000/60000 (87%)]\t0.185630\n",
      "Train Epoch: (epoch) [54000/60000 (90%)]\t0.215364\n",
      "Train Epoch: (epoch) [56000/60000 (93%)]\t0.201739\n",
      "Train Epoch: (epoch) [58000/60000 (97%)]\t0.082393\n",
      "\n",
      "Test set: Average loss:  0.0013, Accuracy 9662/ 10000 (97%\\)\n",
      "Train Epoch: (epoch) [0/60000 (0%)]\t0.172238\n",
      "Train Epoch: (epoch) [2000/60000 (3%)]\t0.228749\n",
      "Train Epoch: (epoch) [4000/60000 (7%)]\t0.097184\n",
      "Train Epoch: (epoch) [6000/60000 (10%)]\t0.124476\n",
      "Train Epoch: (epoch) [8000/60000 (13%)]\t0.100590\n",
      "Train Epoch: (epoch) [10000/60000 (17%)]\t0.125621\n",
      "Train Epoch: (epoch) [12000/60000 (20%)]\t0.166532\n",
      "Train Epoch: (epoch) [14000/60000 (23%)]\t0.194341\n",
      "Train Epoch: (epoch) [16000/60000 (27%)]\t0.063469\n",
      "Train Epoch: (epoch) [18000/60000 (30%)]\t0.082087\n",
      "Train Epoch: (epoch) [20000/60000 (33%)]\t0.048553\n",
      "Train Epoch: (epoch) [22000/60000 (37%)]\t0.132232\n",
      "Train Epoch: (epoch) [24000/60000 (40%)]\t0.068369\n",
      "Train Epoch: (epoch) [26000/60000 (43%)]\t0.199085\n",
      "Train Epoch: (epoch) [28000/60000 (47%)]\t0.200883\n",
      "Train Epoch: (epoch) [30000/60000 (50%)]\t0.098346\n",
      "Train Epoch: (epoch) [32000/60000 (53%)]\t0.094216\n",
      "Train Epoch: (epoch) [34000/60000 (57%)]\t0.045114\n",
      "Train Epoch: (epoch) [36000/60000 (60%)]\t0.091376\n",
      "Train Epoch: (epoch) [38000/60000 (63%)]\t0.093892\n",
      "Train Epoch: (epoch) [40000/60000 (67%)]\t0.066344\n",
      "Train Epoch: (epoch) [42000/60000 (70%)]\t0.045427\n",
      "Train Epoch: (epoch) [44000/60000 (73%)]\t0.151357\n",
      "Train Epoch: (epoch) [46000/60000 (77%)]\t0.099705\n",
      "Train Epoch: (epoch) [48000/60000 (80%)]\t0.149873\n",
      "Train Epoch: (epoch) [50000/60000 (83%)]\t0.111190\n",
      "Train Epoch: (epoch) [52000/60000 (87%)]\t0.038382\n",
      "Train Epoch: (epoch) [54000/60000 (90%)]\t0.151861\n",
      "Train Epoch: (epoch) [56000/60000 (93%)]\t0.132421\n",
      "Train Epoch: (epoch) [58000/60000 (97%)]\t0.067579\n",
      "\n",
      "Test set: Average loss:  0.0009, Accuracy 9718/ 10000 (97%\\)\n",
      "Train Epoch: (epoch) [0/60000 (0%)]\t0.125546\n",
      "Train Epoch: (epoch) [2000/60000 (3%)]\t0.061383\n",
      "Train Epoch: (epoch) [4000/60000 (7%)]\t0.083481\n",
      "Train Epoch: (epoch) [6000/60000 (10%)]\t0.069312\n",
      "Train Epoch: (epoch) [8000/60000 (13%)]\t0.050880\n",
      "Train Epoch: (epoch) [10000/60000 (17%)]\t0.046407\n",
      "Train Epoch: (epoch) [12000/60000 (20%)]\t0.031064\n",
      "Train Epoch: (epoch) [14000/60000 (23%)]\t0.076338\n",
      "Train Epoch: (epoch) [16000/60000 (27%)]\t0.192201\n",
      "Train Epoch: (epoch) [18000/60000 (30%)]\t0.116382\n",
      "Train Epoch: (epoch) [20000/60000 (33%)]\t0.136184\n",
      "Train Epoch: (epoch) [22000/60000 (37%)]\t0.054823\n",
      "Train Epoch: (epoch) [24000/60000 (40%)]\t0.097366\n",
      "Train Epoch: (epoch) [26000/60000 (43%)]\t0.080762\n",
      "Train Epoch: (epoch) [28000/60000 (47%)]\t0.082608\n",
      "Train Epoch: (epoch) [30000/60000 (50%)]\t0.055082\n",
      "Train Epoch: (epoch) [32000/60000 (53%)]\t0.048585\n",
      "Train Epoch: (epoch) [34000/60000 (57%)]\t0.037589\n",
      "Train Epoch: (epoch) [36000/60000 (60%)]\t0.043418\n",
      "Train Epoch: (epoch) [38000/60000 (63%)]\t0.065954\n",
      "Train Epoch: (epoch) [40000/60000 (67%)]\t0.036337\n",
      "Train Epoch: (epoch) [42000/60000 (70%)]\t0.111063\n",
      "Train Epoch: (epoch) [44000/60000 (73%)]\t0.018736\n",
      "Train Epoch: (epoch) [46000/60000 (77%)]\t0.068295\n",
      "Train Epoch: (epoch) [48000/60000 (80%)]\t0.024691\n",
      "Train Epoch: (epoch) [50000/60000 (83%)]\t0.104461\n",
      "Train Epoch: (epoch) [52000/60000 (87%)]\t0.135705\n",
      "Train Epoch: (epoch) [54000/60000 (90%)]\t0.097680\n",
      "Train Epoch: (epoch) [56000/60000 (93%)]\t0.120399\n",
      "Train Epoch: (epoch) [58000/60000 (97%)]\t0.031101\n",
      "\n",
      "Test set: Average loss:  0.0008, Accuracy 9774/ 10000 (98%\\)\n",
      "Train Epoch: (epoch) [0/60000 (0%)]\t0.054093\n",
      "Train Epoch: (epoch) [2000/60000 (3%)]\t0.109193\n",
      "Train Epoch: (epoch) [4000/60000 (7%)]\t0.067142\n",
      "Train Epoch: (epoch) [6000/60000 (10%)]\t0.031422\n",
      "Train Epoch: (epoch) [8000/60000 (13%)]\t0.039378\n",
      "Train Epoch: (epoch) [10000/60000 (17%)]\t0.049707\n",
      "Train Epoch: (epoch) [12000/60000 (20%)]\t0.031601\n",
      "Train Epoch: (epoch) [14000/60000 (23%)]\t0.094946\n",
      "Train Epoch: (epoch) [16000/60000 (27%)]\t0.067432\n",
      "Train Epoch: (epoch) [18000/60000 (30%)]\t0.092696\n",
      "Train Epoch: (epoch) [20000/60000 (33%)]\t0.029851\n",
      "Train Epoch: (epoch) [22000/60000 (37%)]\t0.109800\n",
      "Train Epoch: (epoch) [24000/60000 (40%)]\t0.089133\n",
      "Train Epoch: (epoch) [26000/60000 (43%)]\t0.065121\n",
      "Train Epoch: (epoch) [28000/60000 (47%)]\t0.077564\n",
      "Train Epoch: (epoch) [30000/60000 (50%)]\t0.100942\n",
      "Train Epoch: (epoch) [32000/60000 (53%)]\t0.111060\n",
      "Train Epoch: (epoch) [34000/60000 (57%)]\t0.074028\n",
      "Train Epoch: (epoch) [36000/60000 (60%)]\t0.061412\n",
      "Train Epoch: (epoch) [38000/60000 (63%)]\t0.020002\n",
      "Train Epoch: (epoch) [40000/60000 (67%)]\t0.073702\n",
      "Train Epoch: (epoch) [42000/60000 (70%)]\t0.070587\n",
      "Train Epoch: (epoch) [44000/60000 (73%)]\t0.086565\n",
      "Train Epoch: (epoch) [46000/60000 (77%)]\t0.071038\n",
      "Train Epoch: (epoch) [48000/60000 (80%)]\t0.024110\n",
      "Train Epoch: (epoch) [50000/60000 (83%)]\t0.038476\n",
      "Train Epoch: (epoch) [52000/60000 (87%)]\t0.050999\n",
      "Train Epoch: (epoch) [54000/60000 (90%)]\t0.059198\n",
      "Train Epoch: (epoch) [56000/60000 (93%)]\t0.040612\n",
      "Train Epoch: (epoch) [58000/60000 (97%)]\t0.059686\n",
      "\n",
      "Test set: Average loss:  0.0007, Accuracy 9780/ 10000 (98%\\)\n",
      "Train Epoch: (epoch) [0/60000 (0%)]\t0.047485\n",
      "Train Epoch: (epoch) [2000/60000 (3%)]\t0.098396\n",
      "Train Epoch: (epoch) [4000/60000 (7%)]\t0.057915\n",
      "Train Epoch: (epoch) [6000/60000 (10%)]\t0.096829\n",
      "Train Epoch: (epoch) [8000/60000 (13%)]\t0.071111\n",
      "Train Epoch: (epoch) [10000/60000 (17%)]\t0.052988\n",
      "Train Epoch: (epoch) [12000/60000 (20%)]\t0.050175\n",
      "Train Epoch: (epoch) [14000/60000 (23%)]\t0.053751\n",
      "Train Epoch: (epoch) [16000/60000 (27%)]\t0.018546\n",
      "Train Epoch: (epoch) [18000/60000 (30%)]\t0.057671\n",
      "Train Epoch: (epoch) [20000/60000 (33%)]\t0.167898\n",
      "Train Epoch: (epoch) [22000/60000 (37%)]\t0.176799\n",
      "Train Epoch: (epoch) [24000/60000 (40%)]\t0.019604\n",
      "Train Epoch: (epoch) [26000/60000 (43%)]\t0.097064\n",
      "Train Epoch: (epoch) [28000/60000 (47%)]\t0.019502\n",
      "Train Epoch: (epoch) [30000/60000 (50%)]\t0.021245\n",
      "Train Epoch: (epoch) [32000/60000 (53%)]\t0.023960\n",
      "Train Epoch: (epoch) [34000/60000 (57%)]\t0.032171\n",
      "Train Epoch: (epoch) [36000/60000 (60%)]\t0.123143\n",
      "Train Epoch: (epoch) [38000/60000 (63%)]\t0.070132\n",
      "Train Epoch: (epoch) [40000/60000 (67%)]\t0.039976\n",
      "Train Epoch: (epoch) [42000/60000 (70%)]\t0.052722\n",
      "Train Epoch: (epoch) [44000/60000 (73%)]\t0.021639\n",
      "Train Epoch: (epoch) [46000/60000 (77%)]\t0.019564\n",
      "Train Epoch: (epoch) [48000/60000 (80%)]\t0.044116\n",
      "Train Epoch: (epoch) [50000/60000 (83%)]\t0.028986\n",
      "Train Epoch: (epoch) [52000/60000 (87%)]\t0.101839\n",
      "Train Epoch: (epoch) [54000/60000 (90%)]\t0.050360\n",
      "Train Epoch: (epoch) [56000/60000 (93%)]\t0.024799\n",
      "Train Epoch: (epoch) [58000/60000 (97%)]\t0.011390\n",
      "\n",
      "Test set: Average loss:  0.0007, Accuracy 9786/ 10000 (98%\\)\n",
      "Train Epoch: (epoch) [0/60000 (0%)]\t0.115958\n",
      "Train Epoch: (epoch) [2000/60000 (3%)]\t0.042334\n",
      "Train Epoch: (epoch) [4000/60000 (7%)]\t0.011358\n",
      "Train Epoch: (epoch) [6000/60000 (10%)]\t0.034559\n",
      "Train Epoch: (epoch) [8000/60000 (13%)]\t0.028872\n",
      "Train Epoch: (epoch) [10000/60000 (17%)]\t0.037523\n",
      "Train Epoch: (epoch) [12000/60000 (20%)]\t0.018665\n",
      "Train Epoch: (epoch) [14000/60000 (23%)]\t0.011924\n",
      "Train Epoch: (epoch) [16000/60000 (27%)]\t0.106810\n",
      "Train Epoch: (epoch) [18000/60000 (30%)]\t0.025298\n",
      "Train Epoch: (epoch) [20000/60000 (33%)]\t0.015352\n",
      "Train Epoch: (epoch) [22000/60000 (37%)]\t0.014889\n",
      "Train Epoch: (epoch) [24000/60000 (40%)]\t0.130502\n",
      "Train Epoch: (epoch) [26000/60000 (43%)]\t0.028091\n",
      "Train Epoch: (epoch) [28000/60000 (47%)]\t0.067517\n",
      "Train Epoch: (epoch) [30000/60000 (50%)]\t0.062068\n",
      "Train Epoch: (epoch) [32000/60000 (53%)]\t0.023093\n",
      "Train Epoch: (epoch) [34000/60000 (57%)]\t0.006419\n",
      "Train Epoch: (epoch) [36000/60000 (60%)]\t0.014532\n",
      "Train Epoch: (epoch) [38000/60000 (63%)]\t0.036619\n",
      "Train Epoch: (epoch) [40000/60000 (67%)]\t0.042866\n",
      "Train Epoch: (epoch) [42000/60000 (70%)]\t0.032901\n",
      "Train Epoch: (epoch) [44000/60000 (73%)]\t0.045812\n",
      "Train Epoch: (epoch) [46000/60000 (77%)]\t0.069359\n",
      "Train Epoch: (epoch) [48000/60000 (80%)]\t0.088438\n",
      "Train Epoch: (epoch) [50000/60000 (83%)]\t0.055417\n",
      "Train Epoch: (epoch) [52000/60000 (87%)]\t0.018137\n",
      "Train Epoch: (epoch) [54000/60000 (90%)]\t0.057777\n",
      "Train Epoch: (epoch) [56000/60000 (93%)]\t0.026644\n",
      "Train Epoch: (epoch) [58000/60000 (97%)]\t0.033182\n",
      "\n",
      "Test set: Average loss:  0.0007, Accuracy 9784/ 10000 (98%\\)\n",
      "Train Epoch: (epoch) [0/60000 (0%)]\t0.011947\n",
      "Train Epoch: (epoch) [2000/60000 (3%)]\t0.064145\n",
      "Train Epoch: (epoch) [4000/60000 (7%)]\t0.078802\n",
      "Train Epoch: (epoch) [6000/60000 (10%)]\t0.025212\n",
      "Train Epoch: (epoch) [8000/60000 (13%)]\t0.024852\n",
      "Train Epoch: (epoch) [10000/60000 (17%)]\t0.011721\n",
      "Train Epoch: (epoch) [12000/60000 (20%)]\t0.037234\n",
      "Train Epoch: (epoch) [14000/60000 (23%)]\t0.024835\n",
      "Train Epoch: (epoch) [16000/60000 (27%)]\t0.023413\n",
      "Train Epoch: (epoch) [18000/60000 (30%)]\t0.036332\n",
      "Train Epoch: (epoch) [20000/60000 (33%)]\t0.018213\n",
      "Train Epoch: (epoch) [22000/60000 (37%)]\t0.008873\n",
      "Train Epoch: (epoch) [24000/60000 (40%)]\t0.033740\n",
      "Train Epoch: (epoch) [26000/60000 (43%)]\t0.018992\n",
      "Train Epoch: (epoch) [28000/60000 (47%)]\t0.045838\n",
      "Train Epoch: (epoch) [30000/60000 (50%)]\t0.077565\n",
      "Train Epoch: (epoch) [32000/60000 (53%)]\t0.038385\n",
      "Train Epoch: (epoch) [34000/60000 (57%)]\t0.031473\n",
      "Train Epoch: (epoch) [36000/60000 (60%)]\t0.077502\n",
      "Train Epoch: (epoch) [38000/60000 (63%)]\t0.021341\n",
      "Train Epoch: (epoch) [40000/60000 (67%)]\t0.058090\n",
      "Train Epoch: (epoch) [42000/60000 (70%)]\t0.039362\n",
      "Train Epoch: (epoch) [44000/60000 (73%)]\t0.024812\n",
      "Train Epoch: (epoch) [46000/60000 (77%)]\t0.024250\n",
      "Train Epoch: (epoch) [48000/60000 (80%)]\t0.038598\n",
      "Train Epoch: (epoch) [50000/60000 (83%)]\t0.057910\n",
      "Train Epoch: (epoch) [52000/60000 (87%)]\t0.057967\n",
      "Train Epoch: (epoch) [54000/60000 (90%)]\t0.022164\n",
      "Train Epoch: (epoch) [56000/60000 (93%)]\t0.062364\n",
      "Train Epoch: (epoch) [58000/60000 (97%)]\t0.005079\n",
      "\n",
      "Test set: Average loss:  0.0006, Accuracy 9792/ 10000 (98%\\)\n",
      "Train Epoch: (epoch) [0/60000 (0%)]\t0.033290\n",
      "Train Epoch: (epoch) [2000/60000 (3%)]\t0.037554\n",
      "Train Epoch: (epoch) [4000/60000 (7%)]\t0.010146\n",
      "Train Epoch: (epoch) [6000/60000 (10%)]\t0.012820\n",
      "Train Epoch: (epoch) [8000/60000 (13%)]\t0.020891\n",
      "Train Epoch: (epoch) [10000/60000 (17%)]\t0.057155\n",
      "Train Epoch: (epoch) [12000/60000 (20%)]\t0.018855\n",
      "Train Epoch: (epoch) [14000/60000 (23%)]\t0.016602\n",
      "Train Epoch: (epoch) [16000/60000 (27%)]\t0.067124\n",
      "Train Epoch: (epoch) [18000/60000 (30%)]\t0.020968\n",
      "Train Epoch: (epoch) [20000/60000 (33%)]\t0.030232\n",
      "Train Epoch: (epoch) [22000/60000 (37%)]\t0.066420\n",
      "Train Epoch: (epoch) [24000/60000 (40%)]\t0.121900\n",
      "Train Epoch: (epoch) [26000/60000 (43%)]\t0.023174\n",
      "Train Epoch: (epoch) [28000/60000 (47%)]\t0.027172\n",
      "Train Epoch: (epoch) [30000/60000 (50%)]\t0.059023\n",
      "Train Epoch: (epoch) [32000/60000 (53%)]\t0.007539\n",
      "Train Epoch: (epoch) [34000/60000 (57%)]\t0.033133\n",
      "Train Epoch: (epoch) [36000/60000 (60%)]\t0.022945\n",
      "Train Epoch: (epoch) [38000/60000 (63%)]\t0.130198\n",
      "Train Epoch: (epoch) [40000/60000 (67%)]\t0.011756\n",
      "Train Epoch: (epoch) [42000/60000 (70%)]\t0.043093\n",
      "Train Epoch: (epoch) [44000/60000 (73%)]\t0.014997\n",
      "Train Epoch: (epoch) [46000/60000 (77%)]\t0.071596\n",
      "Train Epoch: (epoch) [48000/60000 (80%)]\t0.021042\n",
      "Train Epoch: (epoch) [50000/60000 (83%)]\t0.007110\n",
      "Train Epoch: (epoch) [52000/60000 (87%)]\t0.045481\n",
      "Train Epoch: (epoch) [54000/60000 (90%)]\t0.056868\n",
      "Train Epoch: (epoch) [56000/60000 (93%)]\t0.029442\n",
      "Train Epoch: (epoch) [58000/60000 (97%)]\t0.020648\n",
      "\n",
      "Test set: Average loss:  0.0007, Accuracy 9792/ 10000 (98%\\)\n",
      "Train Epoch: (epoch) [0/60000 (0%)]\t0.024317\n",
      "Train Epoch: (epoch) [2000/60000 (3%)]\t0.016703\n",
      "Train Epoch: (epoch) [4000/60000 (7%)]\t0.029010\n",
      "Train Epoch: (epoch) [6000/60000 (10%)]\t0.018049\n",
      "Train Epoch: (epoch) [8000/60000 (13%)]\t0.008711\n",
      "Train Epoch: (epoch) [10000/60000 (17%)]\t0.020801\n",
      "Train Epoch: (epoch) [12000/60000 (20%)]\t0.061099\n",
      "Train Epoch: (epoch) [14000/60000 (23%)]\t0.058972\n",
      "Train Epoch: (epoch) [16000/60000 (27%)]\t0.010600\n",
      "Train Epoch: (epoch) [18000/60000 (30%)]\t0.054032\n",
      "Train Epoch: (epoch) [20000/60000 (33%)]\t0.011484\n",
      "Train Epoch: (epoch) [22000/60000 (37%)]\t0.013769\n",
      "Train Epoch: (epoch) [24000/60000 (40%)]\t0.005983\n",
      "Train Epoch: (epoch) [26000/60000 (43%)]\t0.028118\n",
      "Train Epoch: (epoch) [28000/60000 (47%)]\t0.046149\n",
      "Train Epoch: (epoch) [30000/60000 (50%)]\t0.014755\n",
      "Train Epoch: (epoch) [32000/60000 (53%)]\t0.021171\n",
      "Train Epoch: (epoch) [34000/60000 (57%)]\t0.027550\n",
      "Train Epoch: (epoch) [36000/60000 (60%)]\t0.014412\n",
      "Train Epoch: (epoch) [38000/60000 (63%)]\t0.009026\n",
      "Train Epoch: (epoch) [40000/60000 (67%)]\t0.008803\n",
      "Train Epoch: (epoch) [42000/60000 (70%)]\t0.038459\n",
      "Train Epoch: (epoch) [44000/60000 (73%)]\t0.014339\n",
      "Train Epoch: (epoch) [46000/60000 (77%)]\t0.068705\n",
      "Train Epoch: (epoch) [48000/60000 (80%)]\t0.013102\n",
      "Train Epoch: (epoch) [50000/60000 (83%)]\t0.018700\n",
      "Train Epoch: (epoch) [52000/60000 (87%)]\t0.023140\n",
      "Train Epoch: (epoch) [54000/60000 (90%)]\t0.024727\n",
      "Train Epoch: (epoch) [56000/60000 (93%)]\t0.025851\n",
      "Train Epoch: (epoch) [58000/60000 (97%)]\t0.031565\n",
      "\n",
      "Test set: Average loss:  0.0006, Accuracy 9812/ 10000 (98%\\)\n",
      "Train Epoch: (epoch) [0/60000 (0%)]\t0.060607\n",
      "Train Epoch: (epoch) [2000/60000 (3%)]\t0.035665\n",
      "Train Epoch: (epoch) [4000/60000 (7%)]\t0.025667\n",
      "Train Epoch: (epoch) [6000/60000 (10%)]\t0.054554\n",
      "Train Epoch: (epoch) [8000/60000 (13%)]\t0.015326\n",
      "Train Epoch: (epoch) [10000/60000 (17%)]\t0.017761\n",
      "Train Epoch: (epoch) [12000/60000 (20%)]\t0.020891\n",
      "Train Epoch: (epoch) [14000/60000 (23%)]\t0.010310\n",
      "Train Epoch: (epoch) [16000/60000 (27%)]\t0.008012\n",
      "Train Epoch: (epoch) [18000/60000 (30%)]\t0.044570\n",
      "Train Epoch: (epoch) [20000/60000 (33%)]\t0.006813\n",
      "Train Epoch: (epoch) [22000/60000 (37%)]\t0.017696\n",
      "Train Epoch: (epoch) [24000/60000 (40%)]\t0.028520\n",
      "Train Epoch: (epoch) [26000/60000 (43%)]\t0.016477\n",
      "Train Epoch: (epoch) [28000/60000 (47%)]\t0.042016\n",
      "Train Epoch: (epoch) [30000/60000 (50%)]\t0.060639\n",
      "Train Epoch: (epoch) [32000/60000 (53%)]\t0.027253\n",
      "Train Epoch: (epoch) [34000/60000 (57%)]\t0.041048\n",
      "Train Epoch: (epoch) [36000/60000 (60%)]\t0.013547\n",
      "Train Epoch: (epoch) [38000/60000 (63%)]\t0.018094\n",
      "Train Epoch: (epoch) [40000/60000 (67%)]\t0.022319\n",
      "Train Epoch: (epoch) [42000/60000 (70%)]\t0.044460\n",
      "Train Epoch: (epoch) [44000/60000 (73%)]\t0.013449\n",
      "Train Epoch: (epoch) [46000/60000 (77%)]\t0.042870\n",
      "Train Epoch: (epoch) [48000/60000 (80%)]\t0.013090\n",
      "Train Epoch: (epoch) [50000/60000 (83%)]\t0.041812\n",
      "Train Epoch: (epoch) [52000/60000 (87%)]\t0.005523\n",
      "Train Epoch: (epoch) [54000/60000 (90%)]\t0.069789\n",
      "Train Epoch: (epoch) [56000/60000 (93%)]\t0.024760\n",
      "Train Epoch: (epoch) [58000/60000 (97%)]\t0.042802\n",
      "\n",
      "Test set: Average loss:  0.0007, Accuracy 9808/ 10000 (98%\\)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the training data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_data = MNIST(root='mnist_data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# Calculate mean and std\n",
    "mean = 0.0\n",
    "std = 0.0\n",
    "for images, _ in train_loader:\n",
    "    batch_samples = images.size(0)  # Batch size (the last batch can have smaller size)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "\n",
    "mean /= len(train_loader.dataset)\n",
    "std /= len(train_loader.dataset)\n",
    "\n",
    "mean = mean.numpy()\n",
    "std = std.numpy()\n",
    "\n",
    "# Define mean and std values (assuming they have been calculated earlier)\n",
    "mean = np.array([0.1307], dtype=np.float32)  # Replace with actual mean value\n",
    "std = np.array([0.3081], dtype=np.float32)   # Replace with actual std value\n",
    "\n",
    "# Convert mean and std to GAMSPy parameters\n",
    "mean_param = gp.Parameter(m, name=\"mean\", domain=dim(mean.shape), records=mean)\n",
    "std_param = gp.Parameter(m, name=\"std\", domain=dim(std.shape), records=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gamspy.math.matrix import dim\n",
    "\n",
    "mean = (0.1307,)\n",
    "std = (0.3081,)\n",
    "\n",
    "# Get a single batch of data\n",
    "for data, target in loaders['test']:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    break\n",
    "\n",
    "batch = data.shape[0]\n",
    "\n",
    "# Reshape the input so it matches our declaration in GAMSPy\n",
    "data = data.reshape(batch, -1).T\n",
    "\n",
    "# Reshape the target, labels, so that we can provide them to GAMSPy\n",
    "target_df = pd.DataFrame(target.cpu())\n",
    "target_df[\"val\"] = 1\n",
    "target_df = target_df.pivot(columns=[0], values=\"val\").fillna(0).astype(bool)\n",
    "\n",
    "# Create a container\n",
    "m = gp.Container()\n",
    "\n",
    "# Set epsilon as you wish, higher it is, harder to solve\n",
    "diff_eps = 0.01\n",
    "\n",
    "# Extract weights from CNN model\n",
    "w_conv1_data = model.conv1.weight.cpu().detach().numpy().reshape(10, -1).T  # Flatten the conv layer weights\n",
    "w_fc1_data = model.fc1.weight.cpu().detach().numpy().T\n",
    "init_data = data.cpu().detach().numpy()\n",
    "\n",
    "# Define GAMSPy parameters\n",
    "w_conv1 = gp.Parameter(m, name=\"w_conv1\", domain=dim(w_conv1_data.shape), records=w_conv1_data)\n",
    "w_fc1 = gp.Parameter(m, name=\"w_fc1\", domain=dim(w_fc1_data.shape), records=w_fc1_data)\n",
    "init = gp.Parameter(m, name=\"inp\", domain=dim(init_data.shape), records=init_data)\n",
    "\n",
    "# Variables\n",
    "xn = gp.Variable(m, name=\"xn\", domain=dim((784, batch)))\n",
    "x1 = gp.Variable(m, name=\"x1\", domain=dim((784, batch)))\n",
    "x2 = gp.Variable(m, name=\"x2\", domain=dim((10 * 28 * 28, batch)))\n",
    "x3 = gp.Variable(m, name=\"x3\", domain=dim((10, batch)))\n",
    "a2 = gp.Variable(m, name=\"a2\", domain=dim((10 * 28 * 28, batch)))\n",
    "a3 = gp.Variable(m, name=\"a3\", domain=dim((10, batch)))\n",
    "\n",
    "sample_domain = xn.domain[1]\n",
    "digits_domain = a3.domain[0]\n",
    "\n",
    "target_set = gp.Set(m, name=\"targets\", domain=[sample_domain, digits_domain], records=target_df, uels_on_axes=True)\n",
    "\n",
    "# Assume we will get non-normalized input\n",
    "# This step is important because when we trained our neural network we normalized\n",
    "# with a mean and standard deviation, and here we need to do the same\n",
    "normalize_input = gp.Equation(m, name=\"transform_input\", domain=x1.domain)\n",
    "\n",
    "# Input to the neural network is noise + input image normalized\n",
    "normalize_input[...] = x1[...] == (xn[...] + init[...] - mean[0]) / std[0]\n",
    "\n",
    "# Noise has some limits since neural network was trained with the assumption\n",
    "# that the values are between 0-1 for the input\n",
    "xn.lo[...] = - init[...]\n",
    "xn.up[...] = - init[...] + 1 \n",
    "\n",
    "\n",
    "# Define convolution manually\n",
    "def manual_conv2d(x, w, b, output_shape):\n",
    "    out = np.zeros(output_shape)\n",
    "    batch_size, num_filters, height, width = output_shape\n",
    "    num_channels, kernel_height, kernel_width = w.shape[1:]\n",
    "    \n",
    "    for b_idx in range(batch_size):\n",
    "        for f_idx in range(num_filters):\n",
    "            for h in range(height):\n",
    "                for w in range(width):\n",
    "                    conv_sum = 0\n",
    "                    for c in range(num_channels):\n",
    "                        for kh in range(kernel_height):\n",
    "                            for kw in range(kernel_width):\n",
    "                                h_in = h + kh - kernel_height // 2\n",
    "                                w_in = w + kw - kernel_width // 2\n",
    "                                if 0 <= h_in < height and 0 <= w_in < width:\n",
    "                                    conv_sum += x[c, h_in, w_in, b_idx] * w[f_idx, c, kh, kw]\n",
    "                    out[b_idx, f_idx, h, w] = conv_sum + b[f_idx]\n",
    "    return out\n",
    "\n",
    "# Apply convolution manually\n",
    "conv_out_shape = (batch, 10, 28, 28)\n",
    "conv_out_data = manual_conv2d(x1.cpu().detach().numpy().reshape((1, 28, 28, batch)), \n",
    "                              w_conv1.cpu().detach().numpy().reshape((10, 1, 3, 3)), \n",
    "                              b_conv1.cpu().detach().numpy(), conv_out_shape)\n",
    "conv_out = gp.Parameter(m, name=\"conv_out\", domain=dim(conv_out_shape), records=conv_out_data.reshape(-1))\n",
    "\n",
    "# ReLU activation function\n",
    "calc_activation = gp.Equation(m, name=\"calc_activation\", domain=x2.domain)\n",
    "calc_activation[...] = a2[...] == gp.math.relu(conv_out.reshape(10 * 28 * 28, batch))\n",
    "\n",
    "# Fully connected layer operation\n",
    "calc_fc1 = gp.Equation(m, name=\"calc_fc1\", domain=[w_fc1.domain[1], x2.domain[1]])\n",
    "calc_fc1[...] = a3[...] == w_fc1.T @ a2[...] + b_fc1\n",
    "\n",
    "# Objective and constraints\n",
    "obj = gp.Variable(m, name=\"obj\", domain=[sample_domain])\n",
    "eq_so_far = m.getEquations()\n",
    "\n",
    "results = []\n",
    "result_z = []\n",
    "result_a = []\n",
    "\n",
    "# For every sample we need to solve another optimization problem\n",
    "# to find the minimal vector that changes the label\n",
    "for s in range(batch):\n",
    "    sample_target = int(target[s])\n",
    "    print(f\"sample {s + 1}/{batch}\")\n",
    "\n",
    "    # Ensure the correct label gets less probability than the incorrect labels\n",
    "    make_noise = gp.Equation(m, name=f\"false_label_{s}\", domain=[digits_domain])\n",
    "    make_noise[...] = a3[:, s] >= a3[sample_target, s] + diff_eps\n",
    "    \n",
    "    z = gp.Variable(m, name=\"z\")\n",
    "    specific_equations = [make_noise]\n",
    "\n",
    "    # Pick which norm you would like to use\n",
    "    norm = \"l2\"\n",
    "    if norm == \"l2\":\n",
    "        noise_magnitude = gp.Equation(m, name=f\"noise_magnitude_{s}\")\n",
    "        noise_magnitude[...] = z == gp.math.vector_norm(xn[:, s]) ** 2\n",
    "        specific_equations.append(noise_magnitude)\n",
    "    elif norm == \"linf\":\n",
    "        noise_magnitude_1 = gp.Equation(m, name=f\"noise_magnitude_1_{s}\", domain=xn.domain)\n",
    "        noise_magnitude_2 = gp.Equation(m, name=f\"noise_magnitude_2_{s}\", domain=xn.domain)\n",
    "        noise_magnitude_1[...] = z >= xn[:, s]\n",
    "        noise_magnitude_2[...] = z >= -xn[:, s]\n",
    "        specific_equations.append(noise_magnitude_1)\n",
    "        specific_equations.append(noise_magnitude_2)\n",
    "    \n",
    "    model_noise = gp.Model(\n",
    "        m,\n",
    "        name=\"noise\",\n",
    "        equations=[*eq_so_far, *specific_equations],\n",
    "        problem=\"NLP\",\n",
    "        sense=\"min\",\n",
    "        objective=z,\n",
    "    )\n",
    "\n",
    "    # Solve the optimization problem\n",
    "    model_noise.solve(solver='CONOPT3') \n",
    "    res = xn.records.copy()\n",
    "\n",
    "    noise = np.array(res[res[f\"DenseDim{batch}_1\"] == str(s)].level).reshape(28, 28)\n",
    "    output = a3.records.copy()\n",
    "    \n",
    "    output = np.array(output[output[f\"DenseDim{batch}_1\"] == str(s)].level)\n",
    "    result_a.append(output)\n",
    "    results.append(noise)\n",
    "    result_z.append(z.records.copy().level[0])\n",
    "\n",
    "print(\"Adversarial examples generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Aug 11 2023, 19:44:49) \n[Clang 15.0.0 (clang-1500.0.40.1)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5cb052ee30b1767f3b787a49cdcdd10a9ec652a981c9996512c0650ef1142dee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
