{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "from typing import Tuple\n",
    "\n",
    "import gamspy as gp\n",
    "import gamspy.math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define feed-forward network comprises an input layer of 784 neurons, representing the 784 pixels of the input image, while the hidden layer contains 800 neurons.\n",
    "\n",
    "The output layer has 10 neurons, each corresponding to one of the 10 possible digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_neurons = 20\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(784, hidden_layer_neurons, bias=False)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.l2 = nn.Linear(hidden_layer_neurons, 10, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.reshape(x, (x.shape[0], -1))\n",
    "        x = self.l1(x)\n",
    "        x = self.activation(x)\n",
    "        logits = self.l2(x)\n",
    "        output = F.log_softmax(logits, dim=1)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
    "                    help='number of epochs to train (default: 14)')\n",
    "parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                    help='learning rate (default: 1.0)')\n",
    "parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                    help='Learning rate step gamma (default: 0.7)')\n",
    "parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                    help='quickly check a single pass')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                    help='For Saving the current Model')\n",
    "\n",
    "\n",
    "# you can play with different parameters\n",
    "args = parser.parse_args([\n",
    "    \"--epochs=5\",\n",
    "    \"--log-interval=100\",\n",
    "    \"--test-batch-size=32\" # number of adverserial examples\n",
    "])\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cpu\") # or cuda if you like\n",
    "\n",
    "train_kwargs = {'batch_size': args.batch_size}\n",
    "test_kwargs = {'batch_size': args.test_batch_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.335711\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.390588\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.300583\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.373462\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.261049\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.372822\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.295970\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.383538\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.241864\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.358269\n",
      "\n",
      "Test set: Average loss: 0.2532, Accuracy: 9246/10000 (92%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.164772\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.199850\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.188138\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.267577\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.195545\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.223507\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.224444\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.359290\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.187366\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.289133\n",
      "\n",
      "Test set: Average loss: 0.2327, Accuracy: 9301/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.161778\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.220958\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.146504\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.207764\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.158232\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.185479\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.161233\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.344196\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.152442\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.270242\n",
      "\n",
      "Test set: Average loss: 0.2084, Accuracy: 9380/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.134685\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.198262\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.133518\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.194605\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.168129\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.172123\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.120395\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.335613\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.136711\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.264049\n",
      "\n",
      "Test set: Average loss: 0.1992, Accuracy: 9404/10000 (94%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.131099\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.192082\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.125390\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.183514\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.173555\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.164984\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.094608\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.313582\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.128740\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.255117\n",
      "\n",
      "Test set: Average loss: 0.1940, Accuracy: 9421/10000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean = (0.1307,)\n",
    "std = (0.3081,)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "    ])\n",
    "transform2 = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                          transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False, download=True,\n",
    "                          transform=transform)\n",
    "non_transformed_test = datasets.MNIST('../data', train=False,\n",
    "                                      transform=transform2)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "non_transformed_loader = torch.utils.data.DataLoader(non_transformed_test, **test_kwargs)\n",
    "model = SimpleModel().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating to GAMSPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gamspy.math.matrix import dim\n",
    "\n",
    "# get a single batch of data\n",
    "\n",
    "for data, target in non_transformed_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    break\n",
    "\n",
    "batch = args.test_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape input so it matches our declaration in GAMSPy\n",
    "\n",
    "data = data.reshape(batch,-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 1/32\n",
      "sample 2/32\n",
      "sample 3/32\n",
      "sample 4/32\n",
      "sample 5/32\n",
      "sample 6/32\n",
      "sample 7/32\n",
      "sample 8/32\n",
      "sample 9/32\n",
      "sample 10/32\n",
      "sample 11/32\n",
      "sample 12/32\n",
      "sample 13/32\n",
      "sample 14/32\n",
      "sample 15/32\n",
      "sample 16/32\n",
      "sample 17/32\n",
      "sample 18/32\n",
      "sample 19/32\n",
      "sample 20/32\n",
      "sample 21/32\n",
      "sample 22/32\n",
      "sample 23/32\n",
      "sample 24/32\n",
      "sample 25/32\n",
      "sample 26/32\n",
      "sample 27/32\n",
      "sample 28/32\n",
      "sample 29/32\n",
      "sample 30/32\n",
      "sample 31/32\n",
      "sample 32/32\n"
     ]
    }
   ],
   "source": [
    "from gamspy.math.matrix import dim\n",
    "\n",
    "# Get a single batch of data\n",
    "for data, target in non_transformed_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    break\n",
    "\n",
    "batch = args.test_batch_size\n",
    "\n",
    "# reshape the input so it matches our declaration in GAMSPy\n",
    "data = data.reshape(batch, -1).T\n",
    "\n",
    "# reshape the target, labels, so that we can provide them to GAMSPy\n",
    "target_df = pd.DataFrame(target.cpu())\n",
    "target_df[\"val\"] = 1\n",
    "target_df = target_df.pivot(columns=[0], values=\"val\").fillna(0).astype(bool)\n",
    "\n",
    "\n",
    "# Create a container\n",
    "m = gp.Container()\n",
    "\n",
    "# Set epsilon as you wish, higher it is, harder to solve\n",
    "diff_eps = 0.01\n",
    "\n",
    "w1_data = model.l1.weight.cpu().detach().numpy().T\n",
    "w2_data = model.l2.weight.cpu().detach().numpy().T\n",
    "init_data = data.cpu().detach().numpy()\n",
    "\n",
    "w1 = gp.Parameter(m, name=\"w1\", domain=dim(w1_data.shape), records=w1_data)\n",
    "w2 = gp.Parameter(m, name=\"w2\", domain=dim(w2_data.shape), records=w2_data)\n",
    "init = gp.Parameter(m, name=\"inp\", domain=dim(init_data.shape), records=init_data)\n",
    "\n",
    "# unify how dims are written in the way that 784 vs shapes\n",
    "\n",
    "# x_n stands for noise vector\n",
    "xn = gp.Variable(m, name=\"xn\", domain=dim((784, batch)))\n",
    "x1 = gp.Variable(m, name=\"x1\", domain=dim((784, batch)))\n",
    "x2 = gp.Variable(m, name=\"x2\", domain=dim((hidden_layer_neurons, batch)))\n",
    "x3 = gp.Variable(m, name=\"x3\", domain=dim((10, batch)))\n",
    "a2 = gp.Variable(m, name=\"a2\", domain=dim((hidden_layer_neurons, batch)))\n",
    "a3 = gp.Variable(m, name=\"a3\", domain=dim((10, batch)))\n",
    "\n",
    "sample_domain = xn.domain[1]\n",
    "digits_domain = a3.domain[0]\n",
    "\n",
    "\n",
    "target_set = gp.Set(m, name=\"targets\", domain=[sample_domain, digits_domain], records=target_df, uels_on_axes=True)\n",
    "\n",
    "# Assume we will get non-normalized input\n",
    "# This step is important because when we trained our neural network we normalized\n",
    "# with a mean and standard deviation, and here we need to do the same\n",
    "normalize_input = gp.Equation(m, name=\"transform_input\", domain=x1.domain)\n",
    "\n",
    "# Input to the neural network is noise + input image normalized\n",
    "normalize_input[...] = x1[...] == (xn[...] + init[...] - mean[0]) / std[0]\n",
    "\n",
    "# Noise has some limits since neural network was trained with the assumption\n",
    "# that the values are between 0-1 for the input\n",
    "xn.lo[...] = - init[...]\n",
    "xn.up[...] = - init[...] + 1 \n",
    "\n",
    "calc_mm_1 = gp.Equation(m, name=\"calc_mm_1\", domain=[w1.domain[1], x1.domain[1]])\n",
    "calc_mm_1[...] = a2 == w1.t() @ x1\n",
    "\n",
    "calc_activation = gp.Equation(m, name=\"calc_activation\", domain=x2.domain)\n",
    "calc_activation[...] = x2[...] == gp.math.tanh(a2[...])\n",
    "\n",
    "calc_mm_2 = gp.Equation(m, name=\"calc_mm_2\", domain=[w2.domain[1], x2.domain[1]])\n",
    "calc_mm_2[...] = a3 == w2.t() @ x2 \n",
    "\n",
    "obj = gp.Variable(m, name=\"obj\", domain=[sample_domain])\n",
    "eq_so_far = m.getEquations()\n",
    "\n",
    "results = []\n",
    "result_z = []\n",
    "result_a = []\n",
    "\n",
    "# For every sample we need to solve another optimization problem\n",
    "# to find the minimal vector that changes the label\n",
    "for s in range(batch):\n",
    "    sample_target = int(target[s])\n",
    "    print(\"sample\", f\"{s + 1}/{batch}\")\n",
    "\n",
    "    # Ensure the correct label gets less probability than the incorrect labels\n",
    "    make_noise = gp.Equation(m, name=f\"false_label_{s}\", domain=[digits_domain])\n",
    "    make_noise[digits_domain].where[gp.Ord(digits_domain) != sample_target + 1] = a3[digits_domain, str(s)]  >= a3[str(sample_target), str(s)] + diff_eps\n",
    "    \n",
    "    z = gp.Variable(m, name=\"z\")\n",
    "    specific_equations = [make_noise]\n",
    "\n",
    "    # pick which norm you would like to use\n",
    "    norm = \"l2\"\n",
    "    if norm == \"l2\":\n",
    "        noise_magnitude = gp.Equation(m, name=f\"noise_magnitude_{s}\")\n",
    "        noise_magnitude[...] = z == gp.math.vector_norm(xn[:, str(s)]) ** 2  # TODO gp.math.vector_norm(xn)\n",
    "        specific_equations.append(noise_magnitude)\n",
    "    elif norm == \"linf\":\n",
    "        noise_magnitude_1 = gp.Equation(m, name=f\"noise_magnitude_1_{s}\", domain=xn.domain)\n",
    "        noise_magnitude_2 = gp.Equation(m, name=f\"noise_magnitude_2_{s}\", domain=xn.domain)\n",
    "        noise_magnitude_1[xn.domain[0], str(s)] = z >=  xn[xn.domain[0], str(s)]\n",
    "        noise_magnitude_2[xn.domain[0], str(s)] = z >= -xn[xn.domain[0], str(s)]\n",
    "        specific_equations.append(noise_magnitude_1)\n",
    "        specific_equations.append(noise_magnitude_2)\n",
    "    \n",
    "    \n",
    "    model_noise = gp.Model(\n",
    "        m,\n",
    "        name=\"noise\",\n",
    "        equations=[*eq_so_far, *specific_equations],\n",
    "        problem=\"NLP\",\n",
    "        sense=\"min\",\n",
    "        objective=z,\n",
    "    )\n",
    "\n",
    "    # Knitro is a local MINLP solver, so we will get a local optima\n",
    "    model_noise.solve(solver='CONOPT3') # output=sys.stdout if you like to show the log from the solver\n",
    "    res = xn.records.copy()\n",
    "\n",
    "    noise = np.array(res[res[f\"DenseDim{batch}_1\"] == str(s)].level).reshape(28, 28)\n",
    "    # copy newly predicted output by GAMS\n",
    "    output = a3.records.copy()\n",
    "    \n",
    "    output = np.array(output[output[f\"DenseDim{batch}_1\"] == str(s)].level)\n",
    "    result_a.append(output)\n",
    "    # store the noise\n",
    "    results.append(noise)\n",
    "    # store the norm of the noise\n",
    "    result_z.append(z.records.copy().level[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5cb052ee30b1767f3b787a49cdcdd10a9ec652a981c9996512c0650ef1142dee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
